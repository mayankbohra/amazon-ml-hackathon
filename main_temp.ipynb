{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3796b9",
   "metadata": {},
   "source": [
    "# Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955c5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytesseract\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from src.utils import download_images\n",
    "from src.constants import entity_unit_map, allowed_units\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e199c",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3effae",
   "metadata": {},
   "source": [
    "## Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d09a8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet.eval()\n",
    "\n",
    "def preprocess_for_resnet(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img_tensor = transform(img).unsqueeze(0) \n",
    "        return img_tensor\n",
    "    except (OSError, UnidentifiedImageError) as e:\n",
    "        return None\n",
    "\n",
    "def extract_features(image_path):\n",
    "    img_tensor = preprocess_for_resnet(image_path)\n",
    "    if img_tensor is None:\n",
    "        return None  \n",
    "    with torch.no_grad():\n",
    "        features = resnet(img_tensor)\n",
    "    return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f78631",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f35a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        extracted_text = pytesseract.image_to_string(gray_image)\n",
    "        return extracted_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {image_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a97909",
   "metadata": {},
   "source": [
    "## Combining Features and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2daffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_and_text(features, text):\n",
    "    text_features = vectorizer.transform([text]).toarray()\n",
    "    combined_features = np.concatenate((features, text_features.flatten()))\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831a772",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea544cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []  \n",
    "y = []  \n",
    "\n",
    "def train_model(npz_file_path):\n",
    "    data = np.load(npz_file_path)\n",
    "    X = data['X_train']  \n",
    "    y = data['y_train']\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "    print(f\"Model F1 score: {f1}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73883e",
   "metadata": {},
   "source": [
    "## Check Image Downloads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5916bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images_already_downloaded(image_links, image_folder):\n",
    "    print(\"Checking if images are already downloaded...\")\n",
    "    missing_images = []\n",
    "    for image_link in tqdm(image_links, desc=\"Checking images\"):\n",
    "        image_filename = os.path.basename(image_link)\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            missing_images.append(image_link)\n",
    "    print(f\"Check completed. {len(missing_images)} missing images found.\")\n",
    "    return missing_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898fd9b",
   "metadata": {},
   "source": [
    "## Prediction Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e947ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = 'model.pkl'\n",
    "# with open(model_filename, 'rb') as file:\n",
    "#     model = pickle.load(file)\n",
    "\n",
    "def predictor(image_link, category_id, entity_name, model, image_folder):\n",
    "    image_filename = os.path.basename(image_link)\n",
    "    image_path = os.path.join(image_folder, image_filename)\n",
    "    \n",
    "    print(f\"Processing image link: {image_link}\")\n",
    "    print(f\"Image will be saved at: {image_path}\")\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found locally. Downloading image: {image_filename}\")\n",
    "        download_images([image_link], image_folder, allow_multiprocessing=False)\n",
    "        print(f\"Image downloaded: {image_filename}\")\n",
    "    \n",
    "    print(f\"Extracting features from image: {image_filename}\")\n",
    "    features = extract_features(image_path)\n",
    "\n",
    "    if features is None:\n",
    "        print(f\"Skipping prediction due to incomplete or corrupt image: {image_filename}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Making prediction for entity: {entity_name}\")\n",
    "    prediction = model.predict([features])[0]\n",
    "\n",
    "    if entity_name in entity_unit_map:\n",
    "        entity_units = entity_unit_map[entity_name]\n",
    "        if prediction not in entity_units:\n",
    "            print(f\"Prediction {prediction} is not in the allowed units for entity {entity_name}.\")\n",
    "            return None  \n",
    "    else:\n",
    "        print(f\"Entity name {entity_name} not found in the entity-unit map.\")\n",
    "\n",
    "    print(f\"Prediction completed: {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483c3f",
   "metadata": {},
   "source": [
    "# Main Processing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355877e",
   "metadata": {},
   "source": [
    "## Training Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79c2060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from folder: ./dataset/\n",
      "Training dataset loaded.\n",
      "Fitting the vectorizer on training text data...\n",
      "Vectorizer fitted.\n",
      "Checking if images are already downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 263859/263859 [00:28<00:00, 9290.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check completed. 0 missing images found.\n",
      "All training images are already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_FOLDER = './dataset/'\n",
    "IMAGE_DOWNLOAD_FOLDER_TRAIN = './downloaded_images/'\n",
    "IMAGE_DOWNLOAD_FOLDER_TEST = './downloaded_images_test/'\n",
    "\n",
    "print(f\"Loading training dataset from folder: {DATASET_FOLDER}\")\n",
    "train_data = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "print(\"Training dataset loaded.\")\n",
    "\n",
    "print(\"Fitting the vectorizer on training text data...\")\n",
    "text_data = train_data['entity_value'].fillna('') \n",
    "vectorizer.fit(text_data)\n",
    "print(\"Vectorizer fitted.\")\n",
    "\n",
    "missing_train_images = check_images_already_downloaded(train_data['image_link'].tolist(), IMAGE_DOWNLOAD_FOLDER_TRAIN)\n",
    "\n",
    "if missing_train_images:\n",
    "    print(f\"Found {len(missing_train_images)} images to download for training.\")\n",
    "    download_images(missing_train_images, IMAGE_DOWNLOAD_FOLDER_TRAIN)\n",
    "    print(\"Image download process for training completed.\")\n",
    "else:\n",
    "    print(\"All training images are already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c6d15",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        extracted_text = pytesseract.image_to_string(gray_image)\n",
    "        return extracted_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def combine_features_and_text(features, text):\n",
    "    text_features = vectorizer.transform([text]).toarray()\n",
    "    combined_features = np.concatenate((features, text_features.flatten()))\n",
    "    return combined_features\n",
    "\n",
    "print(\"Starting feature extraction for training images...\")\n",
    "\n",
    "num_images_to_process = 20\n",
    "limited_train_data = train_data.head(num_images_to_process)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "skipped_images = []\n",
    "successful_images = []\n",
    "\n",
    "with tqdm(total=limited_train_data.shape[0], desc='Extracting Features', unit='image', ncols=100) as pbar:\n",
    "    for index, row in limited_train_data.iterrows():\n",
    "        image_link = row['image_link']\n",
    "        entity_value = row['entity_value']\n",
    "\n",
    "        image_filename = os.path.basename(image_link)\n",
    "        image_path = os.path.join(IMAGE_DOWNLOAD_FOLDER_TRAIN, image_filename)\n",
    "\n",
    "        features = extract_features(image_path)\n",
    "\n",
    "        if features is not None:\n",
    "            extracted_text = extract_text_with_ocr(image_path)\n",
    "            combined_input = combine_features_and_text(features, extracted_text)\n",
    "\n",
    "            X_train.append(combined_input)\n",
    "            y_train.append(entity_value)\n",
    "            successful_images.append(image_filename)\n",
    "        else:\n",
    "            skipped_images.append(image_filename)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"Feature extraction for training completed.\")\n",
    "print(f\"Total images processed: {len(limited_train_data)}\")\n",
    "print(f\"Successfully extracted features from {len(successful_images)} images.\")\n",
    "print(f\"Skipped {len(skipped_images)} images.\")\n",
    "\n",
    "if skipped_images:\n",
    "    print(\"Skipped images:\")\n",
    "    for img in skipped_images:\n",
    "        print(f\" - {img}\")\n",
    "\n",
    "# if X_train:\n",
    "#     sample_features = np.array(X_train)\n",
    "#     print(\"Sample of extracted features:\")\n",
    "#     print(sample_features[:5])  # Print the first 5 feature vectors\n",
    "#     print(\"Feature vector shape:\", sample_features[0].shape)\n",
    "# else:\n",
    "#     print(\"No features were extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76a05c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e3f7f99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on extracted features...\n",
      "Model accuracy: 0.105\n",
      "Model F1 score: 0.08382142857142856\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model on extracted features...\")\n",
    "model = train_model(\"combined_input.npz\")\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f051b1e",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc55b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Loading test dataset from folder: {DATASET_FOLDER}\")\n",
    "test_data = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "print(\"Test dataset loaded.\")\n",
    "\n",
    "missing_test_images = check_images_already_downloaded(test_data['image_link'].tolist(), IMAGE_DOWNLOAD_FOLDER_TEST)\n",
    "\n",
    "if missing_test_images:\n",
    "    print(f\"Found {len(missing_test_images)} images to download for testing.\")\n",
    "    download_images(missing_test_images, IMAGE_DOWNLOAD_FOLDER_TEST)  \n",
    "    print(\"Image download process for testing completed.\")\n",
    "else:\n",
    "    print(\"All test images are already downloaded.\")\n",
    "\n",
    "print(\"Starting predictions for test data...\")\n",
    "test_predictions = test_data.copy()\n",
    "test_predictions['prediction'] = test_predictions.apply(\n",
    "    lambda row: predictor(row['image_link'], row['group_id'], row['entity_name'], model, IMAGE_DOWNLOAD_FOLDER_TEST), axis=1\n",
    ")\n",
    "\n",
    "test_predictions = test_predictions[test_predictions['prediction'].notnull()]\n",
    "print(\"Predictions for test data completed.\")\n",
    "\n",
    "y_true = test_predictions['entity_name'] \n",
    "y_pred = test_predictions['prediction']\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Model accuracy: {accuracy}\")\n",
    "print(f\"Model F1 score: {f1}\")\n",
    "\n",
    "output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
    "print(f\"Saving predictions to {output_filename}...\")\n",
    "test_predictions[['index', 'prediction']].to_csv(output_filename, index=False)\n",
    "print(f\"Predictions saved to {output_filename}.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
